{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity recognition on the Capture24 dataset\n",
    "\n",
    "## Semi-supervised learning\n",
    "\n",
    "While digital data collection is becoming easier and cheaper, labeling such data\n",
    "still requires expensive and time-consuming human labor.\n",
    "For example, while it is possible to label accelerometer readings for ~150\n",
    "participants as in our Capture24 dataset, it is unfeasible to do so for the\n",
    "currently available *unlabeled* accelerometer measurements of ~100k\n",
    "participants from the UK Biobank, since *a)* compliance to wear a body camera\n",
    "is much lower than a wrist-worn accelerometer and *b)* the human labor to go\n",
    "through all the camera recordings would be very expensive.\n",
    "Semi-supervised learning is therefore of great interest, where the aim is to make use\n",
    "of unlabeled data during training to improve the model performance.\n",
    "\n",
    "###### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import utils\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###### Load dataset and hold out some instances for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of: ['X_feats', 'y', 'pid', 'time', 'annotation']\n",
      "Shape of X_train: (6025, 125)\n",
      "Shape of X_test: (4991, 125)\n"
     ]
    }
   ],
   "source": [
    "# data = np.load('capture24.npz', allow_pickle=True)\n",
    "data = np.load('capture24_small.npz', allow_pickle=True)\n",
    "print(\"Contents of:\", data.files)\n",
    "X, y, pid, time = data['X_feats'], data['y'], data['pid'], data['time']\n",
    "\n",
    "# Hold out some participants for testing the model\n",
    "test_pids = [2, 3]\n",
    "test_mask = np.isin(pid, test_pids)\n",
    "train_mask = ~np.isin(pid, test_pids)\n",
    "X_train, y_train, pid_train = X[train_mask], y[train_mask], pid[train_mask]\n",
    "X_test, y_test, pid_test = X[test_mask], y[test_mask], pid[test_mask]\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest semi-supervised methods is using proxy-labels via self-training. The idea is to simply evaluate a trained model on the unlabeled instances and incorporate those with high confidence predictions into the training set, then re-train the model on the augmented set. This process is repeated several times until some criteria is met, e.g. when no more instances are being included in the training set.\n",
    "This simple technique works well when the initial model is already very strong. If the initial model is weak, however, it may reinforce the mistakes in its predictions.\n",
    "\n",
    "In the following, we train a random forest classifier with self-training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e0e8b00a2044aca4ef378a3fb9339a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2440 instances from the test set\n",
      "Using 3287 instances from the test set\n",
      "Using 3519 instances from the test set\n",
      "Using 3629 instances from the test set\n",
      "Using 3718 instances from the test set\n",
      "Using 3847 instances from the test set\n",
      "Using 3966 instances from the test set\n",
      "Using 4013 instances from the test set\n",
      "Using 4035 instances from the test set\n",
      "Using 4054 instances from the test set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=100, oob_score=True, n_jobs=2)\n",
    "\n",
    "# initial model and predictions\n",
    "classifier.fit(X_train, y_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "y_test_prob = classifier.predict_proba(X_test)\n",
    "y_test_pred_old = None\n",
    "max_iter = 10\n",
    "prob_threshold = 0.8\n",
    "\n",
    "for i in tqdm(range(max_iter)):\n",
    "\n",
    "    if np.array_equal(y_test_pred, y_test_pred_old):\n",
    "        tqdm.write(\"Iteration stopped: no more change found in self-training\")\n",
    "        break\n",
    "\n",
    "    y_test_pred_old = np.copy(y_test_pred)\n",
    "    confident_mask = np.any(y_test_prob > prob_threshold, axis=1)\n",
    "    tqdm.write(f\"Using {np.sum(confident_mask)} instances from the test set\")\n",
    "\n",
    "    # re-train on augmented set\n",
    "    classifier.fit(\n",
    "        np.vstack((X_train, X_test[confident_mask])),\n",
    "        np.hstack((y_train, y_test_pred_old[confident_mask]))\n",
    "    )\n",
    "\n",
    "    # updated predictions\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    y_test_prob = classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Smooth the predictions via HMM and evaluate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random forest performance with self-training and HMM smoothing ---\n",
      "Cohen kappa score: 0.8435334484331689\n",
      "Accuracy score: 0.9085103057100109\n",
      "Confusion matrix:\n",
      " [[1913    5    6    4    0]\n",
      " [  65 2132    0   19    5]\n",
      " [  17   98    0    6    7]\n",
      " [   0   85    0  159   66]\n",
      " [   0   36    0   35  333]]\n"
     ]
    }
   ],
   "source": [
    "Y_oob = classifier.oob_decision_function_[:y_train.shape[0]]\n",
    "prior, emission, transition = utils.train_hmm(Y_oob, y_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "y_test_hmm = utils.viterbi(y_test_pred, prior, transition, emission)\n",
    "print(\"\\n--- Random forest performance with self-training and HMM smoothing ---\")\n",
    "print(\"Cohen kappa score:\", utils.cohen_kappa_score(y_test, y_test_hmm, pid_test))\n",
    "print(\"Accuracy score:\", utils.accuracy_score(y_test, y_test_hmm, pid_test))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_test_hmm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ideas\n",
    "\n",
    "- Incorporate the HMM smoothing into the self-training loop.\n",
    "\n",
    "###### References\n",
    "\n",
    "- [A nice summary of proxy-labels methods](https://ruder.io/semi-supervised/)\n",
    "- [Semi-supervised methods in sklearn](https://scikit-learn.org/stable/modules/label_propagation.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
