{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity recognition on the Capture24 dataset\n",
    "\n",
    "## Semi-supervised learning\n",
    "\n",
    "While digital data collection is becoming easier and cheaper, labeling such data\n",
    "still requires expensive and time-consuming human labor.\n",
    "For example, while it is possible to label accelerometer measurements for ~150\n",
    "participants as in our Capture24 dataset, it is unfeasible to do so for the\n",
    "tens of thousands of *unlabeled* accelerometer measurements that are\n",
    "currently available in the [UK\n",
    "Biobank](https://www.ukbiobank.ac.uk/activity-monitor-3/) because *a)*\n",
    "compliance to wear body cameras is much lower than wrist-worn accelerometers\n",
    "and *b)* the human labor to go through all the camera recordings would be\n",
    "very expensive. Semi-supervised learning is therefore of great interest,\n",
    "where the aim is to somehow use the unlabeled data to improve the model\n",
    "performance.\n",
    "\n",
    "###### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import utils\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###### Load dataset and hold out some instances for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of: ['X_feats', 'y', 'pid', 'time', 'annotation']\n",
      "Shape of X_train: (325619, 125)\n",
      "Shape of X_test: (4991, 125)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('capture24.npz', allow_pickle=True)\n",
    "# data = np.load('capture24_small.npz', allow_pickle=True)\n",
    "print(\"Contents of:\", data.files)\n",
    "X, y, pid, time = data['X_feats'], data['y'], data['pid'], data['time']\n",
    "\n",
    "# Hold out some participants for testing the model\n",
    "test_pids = [2, 3]\n",
    "test_mask = np.isin(pid, test_pids)\n",
    "train_mask = ~np.isin(pid, test_pids)\n",
    "X_train, y_train, pid_train = X[train_mask], y[train_mask], pid[train_mask]\n",
    "X_test, y_test, pid_test = X[test_mask], y[test_mask], pid[test_mask]\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest semi-supervised methods is based on using proxy-labels via self-training. The idea is to simply evaluate a trained model on the unlabeled instances and incorporate those with high confidence predictions into the training set, then re-train the model on the augmented set. This process is repeated several times until some criteria is met, e.g. when no more instances are being included in the training set.\n",
    "This simple technique works well when the initial model is already very strong. If the initial model is weak, however, it may reinforce the mistakes in its predictions.\n",
    "In the following, we train a random forest classifier with self-training.\n",
    "\n",
    "###### Self-training\n",
    "\n",
    "*Note: this takes several minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ff60b0cfc64621b163799a7d4c6a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3250 instances from the test set\n",
      "Using 3445 instances from the test set\n",
      "Using 3527 instances from the test set\n",
      "Using 3589 instances from the test set\n",
      "Using 3628 instances from the test set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=100, oob_score=True, n_jobs=4)\n",
    "\n",
    "# initial model and predictions\n",
    "classifier.fit(X_train, y_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "y_test_prob = classifier.predict_proba(X_test)\n",
    "y_test_pred_old = None\n",
    "max_iter = 5\n",
    "prob_threshold = 0.8\n",
    "\n",
    "for i in tqdm(range(max_iter)):\n",
    "\n",
    "    if np.array_equal(y_test_pred, y_test_pred_old):\n",
    "        tqdm.write(\"Iteration stopped: no more change found in self-training\")\n",
    "        break\n",
    "\n",
    "    y_test_pred_old = np.copy(y_test_pred)\n",
    "    confident_mask = np.any(y_test_prob > prob_threshold, axis=1)\n",
    "    tqdm.write(f\"Using {np.sum(confident_mask)} instances from the test set\")\n",
    "\n",
    "    # re-train on augmented set\n",
    "    classifier.fit(\n",
    "        np.vstack((X_train, X_test[confident_mask])),\n",
    "        np.hstack((y_train, y_test_pred_old[confident_mask]))\n",
    "    )\n",
    "\n",
    "    # updated predictions\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    y_test_prob = classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###### Smooth the predictions via HMM and evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random forest performance with self-training and HMM smoothing ---\n",
      "Cohen kappa score: 0.8333791784064486\n",
      "Accuracy score: 0.9023172029804307\n",
      "Confusion matrix:\n",
      " [[1883   45    0    0    0]\n",
      " [  53 2132    0   26   10]\n",
      " [   0  120    0    2    6]\n",
      " [   0   96    0  120   94]\n",
      " [   0   31    0    5  368]]\n"
     ]
    }
   ],
   "source": [
    "Y_oob = classifier.oob_decision_function_[:y_train.shape[0]]\n",
    "prior, emission, transition = utils.train_hmm(Y_oob, y_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "y_test_hmm = utils.viterbi(y_test_pred, prior, transition, emission)\n",
    "print(\"\\n--- Random forest performance with self-training and HMM smoothing ---\")\n",
    "print(\"Cohen kappa score:\", utils.cohen_kappa_score(y_test, y_test_hmm, pid_test))\n",
    "print(\"Accuracy score:\", utils.accuracy_score(y_test, y_test_hmm, pid_test))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_test_hmm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ideas\n",
    "\n",
    "- Tune acceptance threshold to incorporate high confidence predictions.\n",
    "- Incorporate the HMM smoothing into the self-training loop.\n",
    "\n",
    "###### References\n",
    "\n",
    "- [A nice summary of proxy-labels methods](https://ruder.io/semi-supervised/)\n",
    "- [Semi-supervised methods in sklearn](https://scikit-learn.org/stable/modules/label_propagation.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
