{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BDI_DL_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L76V5gmcO9LK",
        "colab_type": "text"
      },
      "source": [
        "# Code for data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qv0oovON3pL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Ki6Dc6bS9YjueO7_YR_CH4b_eMHQuCyf' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Ki6Dc6bS9YjueO7_YR_CH4b_eMHQuCyf\" -O x_raw_small.npz && rm -rf /tmp/cookies.txt\n",
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1w0SSWeZoMP1r21Xznm2mx9NorTEisnPq' -O small_y_label.npz\n",
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=17NujAC2E68NbuqxBcUaGklEE5z0M_Ukp' -O small_pid.npz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDqN8La-oCQX",
        "colab_type": "text"
      },
      "source": [
        "# **Classifying Time Series Motion data using Deep Learning**\n",
        "\n",
        "In this exercise we are going to apply some Deep Learning techniques to the Capture24 wearable data set. Unlike classical Machine Learning techniques in which we engineer features with which to train our model, we are going to feed the raw sensor data in and let the network learn its own features.\n",
        "\n",
        "First of all, we need to declare the various Numpy and Keras libraries that we'll be using in this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T3wa_ejla5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np          \n",
        "import matplotlib.pyplot as plt  \n",
        "import random\n",
        "import io\n",
        "from datetime import datetime\n",
        "from sklearn import preprocessing\n",
        "import keras\n",
        "from keras.layers import LSTM, Dense, Input, Dropout, Convolution2D, Flatten, Activation, BatchNormalization, Convolution1D, MaxPool2D, MaxPool1D, ConvLSTM2D\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "# configure notebook to display plots\n",
        "%matplotlib inline\n",
        "\n",
        "print('Imports done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6Hf4RQIEGcV",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to define a function that will allow us to look at the results in a confusion matrix format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WtEdKS0x1iks",
        "colab": {}
      },
      "source": [
        "def show_confusion_matrix(predictions, truth):\n",
        "  c_matrix = np.zeros((5,5), dtype=np.int)\n",
        "  for i, pred in enumerate(predictions):\n",
        "    row = np.argmax(pred)\n",
        "    col = np.argmax(truth[i])\n",
        "    if i==0:\n",
        "      print(row, col)\n",
        "    c_matrix[row, col] +=1\n",
        "  print('Predictions - rows, Truth - columns')\n",
        "  print('sleep, sedentary, tasks-light, walking, moderate')\n",
        "  print(c_matrix)\n",
        "\n",
        "print('show_confusion_matrix() created.')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsqJHYBCpflB",
        "colab_type": "text"
      },
      "source": [
        "Let's load some data from the UK biobank wearable activity dataset. For this exercise we are using a subset of the data - to keep the training times shorter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IawchWGmBkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the raw data from a numpy exported file\n",
        "x_raw = np.load('x_raw_small.npz',allow_pickle=True)\n",
        "x_raw = x_raw['arr_0']\n",
        "# load the activity labels that relate to this data from\n",
        "y = np.load('small_y_label.npz',allow_pickle=True)\n",
        "y = y['arr_0']\n",
        "\n",
        "# load the activity labels that relate to this data from\n",
        "pid = np.load('small_pid.npz',allow_pickle=True)\n",
        "pid = pid['arr_0']\n",
        "\n",
        "#Check the data looks okay\n",
        "print('y shape:', y.shape)\n",
        "print('x_raw shape:', x_raw.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb1UINqVuIn9",
        "colab_type": "text"
      },
      "source": [
        "So, you can see that we have 11016 labelled time steps.\n",
        "Each time step is 30 seconds long, with data collected for x, y and z axes 100 times per second, giving the (11016, 3, 3000) shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke3W1tJir3cH",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at some of the data for a random 30 second time step (run the cell again for a different result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IqxS9AMr2TT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Provide a list of activity names that relate to each of the numeric activity labels\n",
        "class_names = ['sleep', 'sedentary', 'tasks-light', 'walking', 'moderate']\n",
        "ix = random.randint(0,100)\n",
        "fig, axs = plt.subplots(5, sharex=True, sharey=True, figsize=(5,5))\n",
        "for i in range(len(class_names)):\n",
        "    axs[i].plot(x_raw[y == i][ix].T)\n",
        "    axs[i].set_title(class_names[i])\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJCQZkd70k0Y",
        "colab_type": "text"
      },
      "source": [
        "So, we can see that, visually, sleep is quite easy to recognise, whereas the distinction between some of the other classes is a little more subtle.\n",
        "\n",
        "Next, let's look at the distribution of the activities within our data. Can you add one or more plots that show the same information for each of the five unique PIDs in the detaset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b62otHz_1N8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot a bar chart of each activity from all entries \n",
        "fig, ax = plt.subplots()\n",
        "x = np.arange(5)\n",
        "plt.bar(x, sum(keras.utils.to_categorical(y)))\n",
        "plt.xticks(x, class_names)\n",
        "plt.show()\n",
        "\n",
        "# TODO Add further plot(s) that show the breakdown of activities for each participant\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l9ag_87gNvoS"
      },
      "source": [
        "Okay, so we can see that we have a lot more 'sleep' and 'sedentary' data than we do the other categories. This is something that you need to consider when training a model (Try searching for 'class imbalance', if you are not sure)\n",
        "\n",
        "Before we start training, we need to split our data into training and validation sets. In this case we are using 3 participants for training and the other 2 for validation. The data is in participant order so we can split it at the transition between person 3 and person 4, which is up to record number 7711 of our 11016. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLdPWmMhMKZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Optional - shuffle the data prior to splitting\n",
        "rng = np.random.random.__self__\n",
        "indexes = np.array(range(11016), dtype=int)\n",
        "rng.shuffle(indexes)\n",
        "y = y[indexes]\n",
        "x_raw = x_raw[indexes]\n",
        "\n",
        "# Split the y labels data between train and validation\n",
        "testy = y[-3305:]\n",
        "trainy = y[:7711]\n",
        "\n",
        "# Do the same for the sensor data\n",
        "testx = x_raw[-3305:]\n",
        "trainx = x_raw[:7711]\n",
        "\n",
        "print(trainy.shape, testy.shape, trainx.shape, testx.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVYMKUU0NmOa",
        "colab_type": "text"
      },
      "source": [
        "The next thing that we need to do is to get our labels into a format suitable for Deep Learning training. At the moment the y labels are values ranging from 0 to 4, representing the five activity classes. Instead, they need to be in the One-Hot-Encoded format e.g. [1,0,0,0,0] = 0 and [0,0,0,1,0] = 4. This format matches the output of the network, which is making a prediction for each of the classes and enables the DL framework (Tensorflow/Keras) to calculate the loss for us (Hint - Keras has a function for this :))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACYzYzDkmRnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO - one hot encode the class values\n",
        "trainy = ....\n",
        "testy = ....\n",
        "\n",
        "print(trainy.shape, testy.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugs6zqhKpwUN",
        "colab_type": "text"
      },
      "source": [
        "Lets take a look at one of the thirty second time steps, from a random interval. You can see the three accelerometer plots over the 30 second period, along with the activity label (run again for a different time step)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnbtYwmbdvLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ix = random.randint(0,1000)\n",
        "plt.plot(trainx[200,0,:], label='x')\n",
        "plt.plot(trainx[200,1,:], label='y')\n",
        "plt.plot(trainx[200,2,:], label='z')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "class_names[y[200]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMcF9c4qrSa_",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the data in a format that we can ingest, it's time to try perhaps the most simple Deep Learning approach: A Multi-Layer Perceptron.\n",
        "This type of neural network is very flexible but is not ideal for all tasks. Compared to other approaches, especially for image-type inputs, it is quite memory hungry because every neuron is attached to every other neuron in the subsequent layer (i.e. it is Fully-Connected). However, its simplicity and flexibility make it a great starting point.\n",
        "\n",
        "First, let's create a function that will train any model using the test and training data that we pass as parameters and display the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wSVC32C8e9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, inputs, test_inputs, epochs=100, class_weights=None):\n",
        "  # train the model\n",
        "  history = model.fit(inputs, trainy, epochs=epochs, batch_size=256, validation_data=(test_inputs, testy), verbose=2, shuffle=False, class_weight=class_weights)\n",
        "\n",
        "  # validate the model on unseeen data\n",
        "  accuracy = model.evaluate(test_inputs, testy, batch_size=256, verbose=0)\n",
        "\n",
        "  # plot history\n",
        "  plt.plot(history.history['acc'], label='train')\n",
        "  plt.plot(history.history['val_acc'], label='test')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  show_confusion_matrix(model.predict(test_inputs), testy)\n",
        "\n",
        "print('train_model() created.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDW0TP3Z9et3",
        "colab_type": "text"
      },
      "source": [
        "For our model, we're using Keras becasuse it only requires a few lines of code to create a network. This will use Tensorflow as the DL platform backend.\n",
        "\n",
        "Note that we need to reshape the input data to a 1 dimensional array for this type of network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYsyV8ZIvWbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a network\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(9000,)))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "# TODO reshape the data to match the network input dimensions\n",
        "inputx = trainx.reshape(...\n",
        "test_inputx = testx.reshape(...\n",
        "\n",
        "# Now run the training\n",
        "train_model(model, inputx, test_inputx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3X2DQBmxRXK",
        "colab_type": "text"
      },
      "source": [
        "After 100 epochs, you should end up with a validation accuracy of somewhere around 50% which is certainly better than chance, but there is a lot of room for improvement. Also, if you run it again note that you may get quite different results.\n",
        "\n",
        "Firstly, how do the plots for training accuracy and validation accuracy differ?\n",
        "Try running the training for 200 epochs by modifying the code below and see whether the validation and training accuracies stabilise. \n",
        "Is there any noticable difference between the training and validation accuracies? Why might this be? \n",
        "\n",
        "When we train with small datasets there is always the risk that the network will overfit the data. It learns to associate each input with its label rather than learning the essence of the task. This means that it tends to perform significantly worse on unseen data. In other words, it does not *generalise* well. To combat this, we can try including a dropout layer. \n",
        "\n",
        "A dropout layer randomly switches certain neurons off, which makes the network less reliant on specific features and therefore less likely to overfit. Note that the dropout rate affects the probability than a neuron will be switched off, so a value closer to 1 will mean more fewer neurons are switched off and, closer, to zero more are left on. Try adjusting this setting to see what happens.\n",
        "\n",
        "Finally, what happens if you add more Dense layers? Experiment with different layers and dropout settings and see what sort of accuracy you can acheive. You should be able to get around 70%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQZG_Za0MJRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a network\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, activation='relu', input_shape=(9000,)))\n",
        "# TODO Add a dropout layer...\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "# reshape the data to match the network input dimensions\n",
        "inputx = trainx.reshape((trainx.shape[0],9000))\n",
        "test_inputx = testx.reshape((testx.shape[0],9000))\n",
        "\n",
        "# Now run the training\n",
        "train_model(model, inputx, test_inputx, epochs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZv6JPASAH_e",
        "colab_type": "text"
      },
      "source": [
        "One thing that you may have noticed is that the network has a tendency not to  predict the 'tasks-light' activity - or in fact 'walking' and 'moderate'. This is very likely because our training data does not contain as many examples for these categories and so the network has learned that a prediction in these categories is much less likely. This a form of bias that it is usually a good idea to avoid. To redress this situation we have two options: \n",
        "We can supplement the dataset with more examples from the less frequent classes or;\n",
        "We can weight the samples according to their frequency, so that the less frequent examples are given more significance. \n",
        "The latter is what we will do in this case. Happily, Keras has a training parameter that we can use named, unsuprisingly, 'class_weight=...', which takes a dictionary in the form: {0 : 4.0, 1 : 3.0, 2 : 20.0, 3 : 12.5, 4 : 1.0} "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8IJ4Tm6Bk0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create function to set the weights according to the distribution of the classes\n",
        "def set_weights():\n",
        "  # TODO - create a weighting function that redresses the class imbalance\n",
        "  # It needs to return a dictionary e.g. {0: 0.1, 1: 0.25, 3: 1.0 ...}\n",
        "  weights = ...\n",
        "  \n",
        "\n",
        "  return weights\n",
        "\n",
        "# create a network\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, activation='relu', input_shape=(9000,)))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "# reshape the data to match the network input dimensions\n",
        "inputx = trainx.reshape((trainx.shape[0],9000))\n",
        "test_inputx = testx.reshape((testx.shape[0],9000))\n",
        "\n",
        "# Now retain\n",
        "train_model(model, inputx, test_inputx, epochs=100, class_weights=set_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSi7BuuCGV16",
        "colab_type": "text"
      },
      "source": [
        "You should observe that the model is now more likely to pick from the rarer classes than before. Feel free to spend more time tweaking your model if you like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKQSIO6aTpWV",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "So, the MLP got us so far, but there are better techniques for this type of data. LSTMs (Long Short Term Memory) are a type of recurrent neural network (RNN) that are able to learn important associations between data points separated by longer time periods than other RNNs. Let's see how they perform on this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i04ceJm9A-ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# design network\n",
        "model = keras.Sequential()\n",
        "model.add(LSTM(500, input_shape=(trainx.shape[1], trainx.shape[2])))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "# TODO you will need to reshape the inputs for this model\n",
        "# reshape the data to match the network input dimensions\n",
        "inputx = trainx.reshape(....\n",
        "test_inputx = testx.reshape(....\n",
        "\n",
        "# Train the model\n",
        "train_model(model, inputx, test_inputx, epochs=100, class_weights=set_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bw-1FssaDv8",
        "colab_type": "text"
      },
      "source": [
        "So we have a slight improvement over the MLP approach although nothing ground-breaking. This is most likely because, although this is time series data, there are few longer-term dependencies in the data. Also we are not doing LSTMs are good at predicting the next values in a sequence and this is actually a classification task. Given that we can easily ingest each time step of 9000 data-points we can actually formulate this task as an image recognition problem. To do this we can make the time axis a spatial axis and then make the three channels into another axis. Alternatively we can make it a 1D image, with pixel intensities as the sensor values and have three channels as input. The latter is the most intuitive approach. In either case, we can then use convolutional kernels rather than fully connected layers to learn a set of features. We can still use a dense layer with softmax outputs for the classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJVHKSVHk28B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# design network\n",
        "model = Sequential()\n",
        "model.add(Convolution1D(32, kernel_size=(5), activation='relu', input_shape=(3000,3) ))\n",
        "# TODO Add some layers...\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "# reshape the data\n",
        "inputx = trainx.reshape((trainx.shape[0],3000,3))\n",
        "test_inputx = testx.reshape((testx.shape[0],3000,3))\n",
        "\n",
        "# Train the model\n",
        "train_model(model, inputx, test_inputx, epochs=100, class_weights=set_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAmEfKk3lx5y",
        "colab_type": "text"
      },
      "source": [
        "So, you should be seeing accuracy well into the 80s% using this convolutional approach. Try experimenting with the number of layers, filter size and adding pooling layers and see how accurate you can get it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOn_rcqgmc0J",
        "colab_type": "text"
      },
      "source": [
        "# ConvLSTM\n",
        "\n",
        "To complete our journey through the approaches for this sort of data, we are finnaly going to try a Convolutional LSTM, which is very mach as it sounds - we use a convolutional kernel and then feed that into the LSTM. It's very simple to set up with Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtdDuBOTodjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# design network\n",
        "model = Sequential()\n",
        "model.add(ConvLSTM2D(filters=32, kernel_size=(1,9), activation='relu', input_shape=(30,1,100,3) ))\n",
        "# TODO Add some layers\n",
        "model.add(Dense(5))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "# TODO reshape the data\n",
        "inputx = trainx.reshape(...\n",
        "test_inputx = testx.reshape(...\n",
        "\n",
        "# Train the model\n",
        "train_model(model, inputx, test_inputx, epochs=150, class_weights=set_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_t6wr7rq9Hb",
        "colab_type": "text"
      },
      "source": [
        "Hopefully this has given you some ideas on how to tackle time series data using Deep Learning. Keras has taken care of many of the implementation details for us, but you will find that other popular DL frameworks operate in quite similar ways.\n",
        "\n",
        "You are encouraged to experiment with these techniques for the rest of the session. Can you you acheive better results? We know that engineered features can acheive 85-90% accuracy, so there is certainly scope for further improvement!\n",
        "\n",
        "Thanks for your attention!\n"
      ]
    }
  ]
}