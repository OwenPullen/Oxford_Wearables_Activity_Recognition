{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Random forest + temporal models\n","\n","In this section, we will train a random forest on the extracted windows\n","from the previous section. We will explore ways to account for the temporal\n","dependency such as mode smoothing and hidden Markov models.\n","\n","## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from mmap import mmap\n","import os\n","import numpy as np\n","import pandas as pd\n","import scipy.stats as stats\n","from imblearn.ensemble import BalancedRandomForestClassifier\n","from sklearn import metrics\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from tqdm.auto import tqdm\n","import utils  # helper functions -- check out utils.py\n","\n","# For reproducibility\n","np.random.seed(42)\n",""]},{"cell_type":"markdown","metadata":{},"source":["## Load dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Path to your extracted windows\n","DATASET_PATH = 'processed_data/'\n","X_FEATS_PATH = 'X_feats.pkl'  # path to your extracted features, if have one\n","print(f'Content of {DATASET_PATH}')\n","print(os.listdir(DATASET_PATH))\n","\n","X = np.load(DATASET_PATH+'X.npy', mmap_mode='r')\n","Y = np.load(DATASET_PATH+'Y.npy')\n","T = np.load(DATASET_PATH+'T.npy')\n","pid = np.load(DATASET_PATH+'pid.npy')\n","X_feats = pd.read_pickle('X_feats.pkl')\n","\n","# As before, let's map the text annotations to simplified labels\n","ANNO_LABEL_DICT_PATH = 'capture24/annotation-label-dictionary.csv'\n","anno_label_dict = pd.read_csv(ANNO_LABEL_DICT_PATH, index_col='annotation', dtype='string')\n","Y = anno_label_dict.loc[Y, 'label:Willetts2018'].to_numpy()\n",""]},{"cell_type":"markdown","metadata":{},"source":["## Train/test split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Hold out participants P101-P151 for testing (51 participants)\n","test_ids = [f'P{i}' for i in range(101,152)]\n","mask_test = np.isin(pid, test_ids)\n","mask_train = ~mask_test\n","X_train, Y_train, T_train, pid_train = \\\n","    X_feats[mask_train], Y[mask_train], T[mask_train], pid[mask_train]\n","X_test, Y_test, T_test, pid_test = \\\n","    X_feats[mask_test], Y[mask_test], T[mask_test], pid[mask_test]\n","print(\"Shape of X_train:\", X_train.shape)\n","print(\"Shape of X_test:\", X_test.shape)\n",""]},{"cell_type":"markdown","metadata":{},"source":["## Train a random forest classifier\n","\n","*Note: this may take a while*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Argument oob_score=True to be used for HMM smoothing (see later below)\n","clf = BalancedRandomForestClassifier(\n","    n_estimators=2000,\n","    replacement=True,\n","    sampling_strategy='not minority',\n","    oob_score=True,\n","    n_jobs=4,\n","    random_state=42,\n","    verbose=1\n",")\n","clf.fit(X_train, Y_train)\n",""]},{"cell_type":"markdown","metadata":{},"source":["## Model performance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","Y_test_pred = clf.predict(X_test)\n","print('\\nClassifier performance')\n","print('Out of sample:\\n', metrics.classification_report(Y_test, Y_test_pred))\n",""]},{"cell_type":"markdown","metadata":{},"source":["Overall, the model seems to do well in distinguishing between very inactive\n","periods (\"sit-stand\" and \"sleep\") and very active ones (\"bicycling\"), but there\n","seems to be confusion between the remaining activities.\n","\n","## Plot predicted vs. true activity profiles\n","\n","Using our utility function, let's plot the activity profile for participant\n","`P101`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mask = pid_test == 'P101'\n","fig, axs = utils.plot_compare(T_test[mask],\n","                              Y_test[mask],\n","                              Y_test_pred[mask],\n","                              trace=X_test.loc[mask, 'std'])\n","fig.show()\n",""]},{"cell_type":"markdown","metadata":{},"source":["The profile plots look good at first glance. After all, the majority of\n","activities happen to be of the sedentary type for which the model performs\n","well &mdash; this is reflected by the relatively high `weighted avg` scores in\n","the table report.\n","However, the `macro avg` scores are still low, and we see that the model\n","struggles to classify relevant activities such as bicycling and walking.\n","Moreover, we find some awkward sequences, for example issues with discontinuous\n","sleep periods. This is because the model is only trained to classify each\n","window instance independently and does not account for temporal dependencies.\n","\n","## Accounting for temporal dependency\n","\n","### Rolling mode smoothing\n","Let's use rolling mode smoothing to smooth the model predictions: Pick the\n","most popular label within a rolling time window.\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def mode(alist):\n","    ''' Mode of a list, but return middle element if ambiguous '''\n","    m, c = stats.mode(alist)\n","    m, c = m.item(), c.item()\n","    if c==1:\n","        return alist[len(alist)//2]\n","    return m\n","\n","def rolling_mode(t, y, window_size='100S'):\n","    y_dtype_orig = y.dtype\n","    # Hack to make it work with pandas.Series.rolling()\n","    y = pd.Series(y, index=t, dtype='category')\n","    y_code_smooth = y.cat.codes.rolling(window_size).apply(mode, raw=True).astype('int')\n","    y_smooth = pd.Categorical.from_codes(y_code_smooth, dtype=y.dtype)\n","    y_smooth = np.asarray(y_smooth, dtype=y_dtype_orig)\n","    return y_smooth\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Smooth the predictions of each participant\n","Y_test_pred_smooth = []\n","unqP, indP = np.unique(pid_test, return_index=True)\n","unqP = unqP[np.argsort(indP)]  # keep the order or else we'll scramble our arrays\n","for p in unqP:\n","    mask = pid_test == p\n","    Y_test_pred_smooth.append(rolling_mode(T_test[mask], Y_test_pred[mask]))\n","Y_test_pred_smooth = np.concatenate(Y_test_pred_smooth)\n","\n","print('\\nClassifier performance -- mode smoothing')\n","print('Out of sample:\\n', metrics.classification_report(Y_test, Y_test_pred_smooth))\n","\n","# Check again participant\n","mask = pid_test == 'P101'\n","fig, axs = utils.plot_compare(T_test[mask],\n","                              Y_test[mask],\n","                              Y_test_pred_smooth[mask],\n","                              trace=X_test.loc[mask, 'std'])\n","fig.show()\n",""]},{"cell_type":"markdown","metadata":{},"source":["\n","The simple mode smoothing already improved performance slightly.\n","\n","### Hidden Markov Model\n","\n","A more principled approch is to use a Hidden Markov Model (HMM). Here the random\n","forest predictions are considered as \"observations\" of the \"hidden ground\n","truth\". The emission matrix can be estimated from probabilistic predictions of\n","model, and the transition matrix can be estimated from the ground truth sequence\n","of activities. The prior probabilities can be set as the rates observed in the\n","dataset, or a uniform (uninformative) prior.\n","\n","Check `utils.train_hmm` and `utils.viterbi` for implementationd details.\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Use the convenientely provided out-of-bag probability predictions from the\n","# random forest training process.\n","# QUESTION: Why not Y_train_prob = clf.predict_proba(X_train) ?\n","Y_train_prob = clf.oob_decision_function_  # out-of-bag probability predictions\n","labels = clf.classes_  # need this to know the label order of cols of Y_train_prob\n","hmm_params = utils.train_hmm(Y_train_prob, Y_train, labels)  # obtain HMM matrices/params\n","Y_test_pred_hmm = utils.viterbi(Y_test_pred, hmm_params)  # smoothing\n","print('\\nClassifier performance -- HMM smoothing')\n","print('Out of sample:\\n', metrics.classification_report(Y_test, Y_test_pred_hmm))\n","\n","# Check again participant\n","mask = pid_test == 'P101'\n","fig, ax = utils.plot_compare(T_test[mask],\n","                             Y_test[mask],\n","                             Y_test_pred_hmm[mask],\n","                             trace=X_test.loc[mask, 'std'])\n","fig.show()\n",""]},{"cell_type":"markdown","metadata":{},"source":["HMM further improves the performance scores.\n","\n","## Is a simple logistic regression enough?\n","\n","*Note: this may take a while*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["clf_LR = LogisticRegression(\n","    max_iter=1000,\n","    multi_class='multinomial',\n","    random_state=42)\n","scaler = StandardScaler()\n","pipe = make_pipeline(scaler, clf_LR)\n","pipe.fit(X_train, Y_train)\n","\n","Y_test_pred_LR = pipe.predict(X_test)\n","\n","# HMM smoothing\n","Y_train_LR_prob = pipe.predict_proba(X_train)  # sorry! LR doesn't provide OOB estimates for free\n","labels = pipe.classes_\n","hmm_params_LR = utils.train_hmm(Y_train_LR_prob, Y_train, labels)\n","Y_test_pred_LR_hmm = utils.viterbi(Y_test_pred_LR, hmm_params_LR)  # smoothing\n","\n","print('\\nClassifier performance -- Logistic regression')\n","print('Out of sample:\\n', metrics.classification_report(Y_test, Y_test_pred_LR_hmm))\n","\n","# Check again participant\n","mask = pid_test == 'P101'\n","fig, axs = utils.plot_compare(T_test[mask],\n","                      Y_test[mask],\n","                      Y_test_pred_LR_hmm[mask],\n","                      trace=X_test.loc[mask, 'std'])\n","fig.show()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" The LR model performed well on the easier classes \"sleep\" and \"sit-stand\",\n","but was much worse on all the other classes."]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}