{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Activity recognition on the Capture24 dataset -- Convolutional neural networks\n",
        "\n",
        "*This section assumes familiarity with [PyTorch](https://pytorch.org/)*\n",
        "\n",
        "In this section, instead of using the hand-crafted features, we use a neural network on the raw accelerometer measurements so that we let the neural network automatically learn relevant features for classification.\n",
        "\n",
        "Setup:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import median_filter\n",
        "from sklearn import preprocessing\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from tqdm import tqdm\n",
        "import utils\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "cudnn.benchmark = True"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Load dataset and hold out some instances for testing: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# X_raw = np.memmap('X_raw.dat', dtype='float32', mode='r').reshape(-1,3,3000)\n",
        "X_raw = np.load('X_raw_small.npy')\n",
        "print(\"Raw data shape:\", X_raw.shape)\n",
        "# data = np.load('capture24.npz', allow_pickle=True)\n",
        "data = np.load('capture24_small.npz', allow_pickle=True)\n",
        "print(\"Data contents:\", data.files)\n",
        "y, pid, time = data['y'], data['pid'], data['time']\n",
        "\n",
        "# Hold out some participants for testing the model\n",
        "test_pids = [2, 3]\n",
        "test_mask = np.isin(pid, test_pids)\n",
        "train_mask = ~np.isin(pid, test_pids)\n",
        "X_train, y_train, pid_train = X_raw[train_mask], y[train_mask], pid[train_mask]\n",
        "X_test, y_test, pid_test = X_raw[test_mask], y[test_mask], pid[test_mask]\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A standard procedure is to normalize the input data, which aids the optimization of neural networks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scaler = preprocessing.StandardScaler(copy=False)\n",
        "X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0],-1)).reshape(X_train.shape)\n",
        "X_test = scaler.transform(X_test.reshape(X_test.shape[0],-1)).reshape(X_test.shape)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by defining a simple feed-forward convolutional neural network (CNN):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, output_dim, in_channels, num_filters_base=8):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, num_filters_base,\n",
        "                8, 4, 2, bias=False),  # state shape (N,num_filters_base,750)\n",
        "            nn.BatchNorm1d(num_filters_base),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(num_filters_base, num_filters_base*2,\n",
        "                6, 4, 2, bias=False),  # state shape (N,num_filters_base*2,188)\n",
        "            nn.BatchNorm1d(num_filters_base*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(num_filters_base*2, num_filters_base*4,\n",
        "                8, 4, 2, bias=False),  # state shape (N,num_filters_base*4,47)\n",
        "            nn.BatchNorm1d(num_filters_base*4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(num_filters_base*4, num_filters_base*8,\n",
        "                3, 2, 1, bias=False),  # state shape (N,num_filters_base*8,24)\n",
        "            nn.BatchNorm1d(num_filters_base*8),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(num_filters_base*8, num_filters_base*16,\n",
        "                4, 2, 1, bias=False),  # state shape (N,num_filters_base*16,12)\n",
        "            nn.BatchNorm1d(num_filters_base*16),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(num_filters_base*16, num_filters_base*32,\n",
        "                4, 2, 1, bias=False),  # state shape (N,num_filters_base*32,6)\n",
        "            nn.BatchNorm1d(num_filters_base*32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(num_filters_base*32, num_filters_base*64,\n",
        "                6, 1, 0, bias=False),  # state shape (N,num_filters_base*64,1)\n",
        "            nn.BatchNorm1d(num_filters_base*64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(num_filters_base*64, output_dim,\n",
        "                1, 1, 0, bias=True)  # state shape (N,output_dim,1)\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm1d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cnn(x).squeeze()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " If there is a GPU, let's use it! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    # torch.cuda.set_device(args.device)\n",
        "    print(\"Using {} device: {}\".format(device, torch.cuda.current_device()))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using {}\".format(device))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Define a dataset iterator that will provide the mini-batches during the training, instantiate the neural network, and define the loss function and optimizer: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_iter = 10\n",
        "lr = 1e-4\n",
        "num_filters_base = 8\n",
        "batch_size = 32\n",
        "\n",
        "X_train_torch = torch.from_numpy(X_train.astype('float32')).to(device)\n",
        "y_train_torch = torch.from_numpy(y_train).to(device)\n",
        "def create_dataloader(batch_size=1):\n",
        "    nbatch = X_train_torch.shape[0] // batch_size\n",
        "    idxs = np.random.permutation(np.arange(X_train_torch.shape[0]))\n",
        "    for i in range(nbatch):\n",
        "        _idxs = idxs[i:i+batch_size]\n",
        "        yield X_train_torch[_idxs], y_train_torch[_idxs]\n",
        "\n",
        "cnn = CNN(\n",
        "    output_dim=utils.NUM_CLASSES,\n",
        "    in_channels=X_train_torch.shape[1],\n",
        "    num_filters_base=num_filters_base\n",
        ").to(device)\n",
        "print(cnn)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(cnn.parameters(), lr=lr, amsgrad=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Training via mini-batch gradient descent begins here. We loop through the training set `max_iter` times with the `dataloader` iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "losses = []\n",
        "for i in tqdm(range(max_iter)):\n",
        "    dataloader = create_dataloader(batch_size)\n",
        "    for x, y in dataloader:\n",
        "        cnn.zero_grad()\n",
        "        output = cnn(x)\n",
        "        loss = loss_fn(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "# View loss during training\n",
        "utils.plt.semilogy(median_filter(losses, size=100))\n",
        "utils.plt.savefig('cnn_loss.png')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Evaluate the model on the hold out set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cnn.eval()\n",
        "with torch.no_grad():\n",
        "    output = cnn(torch.from_numpy(X_test.astype('float32')).to(device))\n",
        "    y_test_pred = torch.argmax(F.softmax(output, dim=1), dim=1).cpu().numpy()\n",
        "print(\"Cohen kappa score:\", utils.cohen_kappa_score(y_test, y_test_pred, pid_test))\n",
        "print(\"Accuracy score:\", utils.accuracy_score(y_test, y_test_pred, pid_test))\n",
        "\n",
        "#TODO: HMM smoothing\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}