{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity recognition on the Capture24 dataset\n",
    "\n",
    "## Neural networks for feature engineering\n",
    "\n",
    "*This section assumes familiarity with [PyTorch](https://pytorch.org/)*\n",
    "\n",
    "In this section, instead of using the hand-crafted features, we use a neural network on the raw accelerometer measurements so that we let the neural network automatically learn relevant features for classification.\n",
    "\n",
    "###### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import median_filter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "import utils\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Function to plot dict of scores\n",
    "def plot_scores(scores, xlabel=None, ylabel=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    for key, vals in scores.items():\n",
    "        ax.plot(vals, label=key)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###### Load dataset and hold out some instances for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of capture24.npz: ['X_feats', 'y', 'pid', 'time', 'annotation']\n",
      "Raw data shape: (11016, 3, 3000)\n",
      "Shape of X_train (6025, 3, 3000)\n",
      "Shape of X_test (4991, 3, 3000)\n"
     ]
    }
   ],
   "source": [
    "# data = np.load('capture24.npz', allow_pickle=True)\n",
    "data = np.load('capture24_small.npz', allow_pickle=True)\n",
    "print(\"Contents of capture24.npz:\", data.files)\n",
    "y, pid, time = data['y'], data['pid'], data['time']\n",
    "# X = utils.load_raw('X_raw.dat')\n",
    "X = np.load('X_raw_small.npy')\n",
    "print(\"Raw data shape:\", X.shape)\n",
    "\n",
    "# Hold out some participants for testing the model\n",
    "pids_test = [2, 3]\n",
    "mask_test = np.isin(pid, pids_test)\n",
    "mask_train = ~mask_test\n",
    "y_train, y_test = y[mask_train], y[mask_test]\n",
    "pid_train, pid_test = pid[mask_train], pid[mask_test]\n",
    "X_train = utils.ArrayFromMask(X, mask_train)\n",
    "X_test = utils.ArrayFromMask(X, mask_test)\n",
    "print(\"Shape of X_train\", X_train.shape)\n",
    "print(\"Shape of X_test\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###### Grab a GPU if there is one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using {} device: {}\".format(device, torch.cuda.current_device()))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Architecture design\n",
    "\n",
    "As a baseline model, we use a convolutional neural network (CNN) with a typical pyramid-like structure. The input to the network is a `(N,3,3000)` array, corresponding to `N` instances of raw triaxial measurements of activity. The output of the network is a `(N,5)` array representing predicted *class scores* for each instance.\n",
    "To obtain probabilities, we can pass each row to a softmax. Then to report a class label, we can pick the highest probability in each row. We output class scores instead of class probabilities or labels because the loss function that we will use operates on the scores (see further below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, output_dim=5, in_channels=3, num_filters_init=8):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, num_filters_init,\n",
    "                4, 2, 1, bias=True),  # (n, 1500)\n",
    "            # nn.BatchNorm1d(num_filters_init),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(num_filters_init, num_filters_init,\n",
    "                4, 2, 1, bias=True),  # (n, 750)\n",
    "            nn.BatchNorm1d(num_filters_init),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(num_filters_init, num_filters_init*2,\n",
    "                4, 2, 1, bias=True),  # (n^2, 375)\n",
    "            # nn.BatchNorm1d(num_filters_init*2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(num_filters_init*2, num_filters_init*2,\n",
    "                3, 2, 1, bias=True),  # (n^2, 188)\n",
    "            nn.BatchNorm1d(num_filters_init*2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(num_filters_init*2, num_filters_init*4,\n",
    "                4, 2, 1, bias=True),  # (n^4, 94)\n",
    "            # nn.BatchNorm1d(num_filters_init*4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(num_filters_init*4, num_filters_init*4,\n",
    "                4, 2, 1, bias=True),  # (n^4, 47)\n",
    "            nn.BatchNorm1d(num_filters_init*4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(num_filters_init*4, num_filters_init*8,\n",
    "                3, 2, 1, bias=True),  # (n^8, 24)\n",
    "            # nn.BatchNorm1d(num_filters_init*8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(num_filters_init*8, num_filters_init*8,\n",
    "                4, 2, 1, bias=True),  # (n^8, 12)\n",
    "            nn.BatchNorm1d(num_filters_init*8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(num_filters_init*8, num_filters_init*16,\n",
    "                4, 2, 1, bias=True),  # (n^16, 6)\n",
    "            # nn.BatchNorm1d(num_filters_init*16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(num_filters_init*16, num_filters_init*16,\n",
    "                4, 2, 1, bias=True),  # (n^16, 3)\n",
    "            nn.BatchNorm1d(num_filters_init*16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(num_filters_init*16, num_filters_init*32,\n",
    "                3, 1, 0, bias=True),  # (n^32, 1)\n",
    "            nn.BatchNorm1d(num_filters_init*32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(num_filters_init*32, output_dim,  # (output_dim, 1)\n",
    "                1, 1, 0, bias=True),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm1d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x).view(x.shape[0],-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Helper functions\n",
    "\n",
    "We need a few helper functions:\n",
    "- A data loader that will provide the mini-batches during training.\n",
    "- A helper function that forward-passes the model on a dataset by chunks &mdash; this is simply to prevent the memory from blowing up.\n",
    "- A function to train a Hidden Markov Model (HMM) using the probabilistic predictions of the model for smoothing.\n",
    "- A function that evaluates the model (CNN + HMM) on a dataset, to be used to track the performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(X, y=None, batch_size=1, shuffle=False):\n",
    "    ''' Create a (batch) iterator over the dataset. Alternatively, use PyTorch's\n",
    "    Dataset and DataLoader classes -- See\n",
    "    https://pytorch.org/tutorials/beginner/data_loading_tutorial.html '''\n",
    "    if shuffle:\n",
    "        idxs = np.random.permutation(np.arange(len(X)))\n",
    "    else:\n",
    "        idxs = np.arange(len(X))\n",
    "    for i in range(0, len(idxs), batch_size):\n",
    "        idxs_batch = idxs[i:i+batch_size]\n",
    "        X_batch = X[idxs_batch]\n",
    "        X_batch = torch.from_numpy(X[idxs_batch])\n",
    "        if y is None:\n",
    "            yield X_batch\n",
    "        else:\n",
    "            y_batch = torch.from_numpy(y[idxs_batch])\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "def forward_by_batches(cnn, X):\n",
    "    ''' Forward pass model on a dataset. Do this by batches so that we do\n",
    "    not blow up the memory. '''\n",
    "    Y = []\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        for x in create_dataloader(X, batch_size=1024, shuffle=False):  # do not shuffle here!\n",
    "            x = x.to(device)\n",
    "            Y.append(cnn(x))\n",
    "    cnn.train()\n",
    "    Y = torch.cat(Y)\n",
    "    return Y\n",
    "\n",
    "def train_hmm(cnn, X, y):\n",
    "    ''' Use the probabilistic predictions of the CNN to compute the Hidden\n",
    "    Markov Model parameters '''\n",
    "    Y_pred = forward_by_batches(cnn, X)  # scores\n",
    "    Y_pred = F.softmax(Y_pred, dim=1)  # probabilities\n",
    "    Y_pred = Y_pred.cpu().numpy()  # cast to numpy array\n",
    "    prior, emission, transition = utils.train_hmm(Y_pred, y)\n",
    "    return prior, emission, transition\n",
    "\n",
    "def evaluate_model(cnn, prior, emission, transition, X, y, pid=None):\n",
    "    Y_pred = forward_by_batches(cnn, X)  # scores\n",
    "    Y_pred = F.softmax(Y_pred, dim=1)  # convert to probabilities\n",
    "    y_pred = torch.argmax(Y_pred, dim=1)  # convert to classes\n",
    "    y_pred = y_pred.cpu().numpy()  # cast to numpy array\n",
    "    y_pred = utils.viterbi(y_pred, prior, emission, transition)  # HMM smoothing\n",
    "    kappa = utils.cohen_kappa_score(y, y_pred, pid)\n",
    "    accuracy = utils.accuracy_score(y, y_pred, pid)\n",
    "    return kappa, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hyperparameters, model instantiation, loss function and optimizer\n",
    "\n",
    "Now we set the hyperparameters, instantiate the model, define the loss\n",
    "function (we use cross entropy for multiclass classification) and optimizer\n",
    "(we use AMSGRAD &mdash; a variant of SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv1d(3, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv1d(32, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Conv1d(32, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv1d(128, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv1d(256, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (18): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (21): ReLU(inplace=True)\n",
      "    (22): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (23): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
      "    (26): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv1d(1024, 5, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_filters_init = 32  # initial num of filters -- see class definition\n",
    "in_channels = 3  # num input channels -- equal to 3 for our raw triaxial timeseries\n",
    "output_dim = utils.NUM_CLASSES  # number of classes (sleep, sedentary, etc...)\n",
    "num_epoch = 50  # num of epochs (full loops though the training set) for SGD training\n",
    "lr = 1e-3  # learning rate in SGD\n",
    "batch_size = 32  # size of the mini-batch in SGD\n",
    "\n",
    "cnn = CNN(\n",
    "    output_dim=output_dim,\n",
    "    in_channels=in_channels,\n",
    "    num_filters_init=num_filters_init\n",
    ").to(device)\n",
    "print(cnn)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=lr, amsgrad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training\n",
    "\n",
    "Training via mini-batch gradient descent begins here. We loop through the training set `num_epoch` times with the `dataloader` iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [27:31<10:49, 46.40s/it]"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "kappa_history = {'train':[], 'test':[]}\n",
    "accuracy_history = {'train':[], 'test':[]}\n",
    "for i in tqdm(range(num_epoch)):\n",
    "    dataloader = create_dataloader(X_train, y_train, batch_size, shuffle=True)\n",
    "    for x, target in dataloader:\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        cnn.zero_grad()\n",
    "        output = cnn(x)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging -- cross entropy loss\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Evalutate performance at the end of each epoch (full loop through the\n",
    "    # training set). We could also do this at every iteration, but this would\n",
    "    # be very expensive since we are evaluating on the entire dataset.\n",
    "    # Aditionally, at the end of each epoch we train a Hidden Markov Model to\n",
    "    # smooth the predictions of the CNN.\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    prior, emission, transition = train_hmm(cnn, X_train, y_train)\n",
    "\n",
    "    # Logging -- performance on train set\n",
    "    kappa, accuracy = evaluate_model(\n",
    "        cnn, prior, emission, transition, X_train, y_train, pid_train)\n",
    "    kappa_history['train'].append(kappa)\n",
    "    accuracy_history['train'].append(accuracy)\n",
    "    # Logging -- performance on test set\n",
    "    kappa, accuracy = evaluate_model(\n",
    "        cnn, prior, emission, transition, X_test, y_test, pid_test)\n",
    "    kappa_history['test'].append(kappa)\n",
    "    accuracy_history['test'].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###### Plot score and loss history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss history\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(median_filter(loss_history, size=100))  # smooth for visualization\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_xlabel('iteration')\n",
    "fig.show()\n",
    "fig.savefig('cnn_loss.png')\n",
    "\n",
    "# Score history\n",
    "fig, _ = plot_scores(kappa_history, xlabel='epoch', ylabel='kappa')\n",
    "fig.show()\n",
    "fig.savefig('cnn_kappa_scores.png')\n",
    "fig, _ = plot_scores(accuracy_history, xlabel='epoch', ylabel='accuracy')\n",
    "fig.show()\n",
    "fig.savefig('cnn_accuracy_scores.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ideas\n",
    "- Implement an early stopping mechanism to select the model at its best out-of-sample performance. Do we track kappa or accuracy?\n",
    "- As an attempt to remove gravity, try detrending the signal. You can implement this in the dataloader:\n",
    "\n",
    "   ```python\n",
    "   # Do this before torch.from_numpy()\n",
    "   X_batch = scipy.signal.detrend(X_batch, type='constant')\n",
    "   ```\n",
    "\n",
    "   See [scipy.signal.detrend](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.detrend.html).\n",
    "\n",
    "- Add a 4th channel containing the vector magnitude, i.e. $(x,y,z,r)$ where $r = \\sqrt{x^2+y^2+z^2}$. You can implement this in the dataloader:\n",
    "\n",
    "   ```python\n",
    "   # Do this before torch.from_numpy()\n",
    "   X_batch = np.concatenate((X_batch, np.linalg.norm(X_batch, axis=1, keepdims=True)), axis=1)\n",
    "   ```\n",
    "\n",
    "- Consider using [spherical coordinates](https://en.wikipedia.org/wiki/Spherical_coordinate_system), or adding them as additional channels.\n",
    "- Modify the architecture. Feel free to use recent architectures such as ResNet.\n",
    "\n",
    "###### References\n",
    "\n",
    "- [A recipe for training neural networks](http://karpathy.github.io/2019/04/25/recipe/)\n",
    "- [PyTorch's Conv1d module](https://pytorch.org/docs/stable/nn.html#conv1d)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
