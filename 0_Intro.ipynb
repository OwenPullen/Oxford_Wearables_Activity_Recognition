{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Activity recognition on the Capture-24 dataset\n","\n","<img src=\"wrist_accelerometer.jpg\" width=\"300\"/>\n","\n","The Capture-24 dataset contains wrist-worn accelerometer data\n","collected from 151 participants. To obtain ground truth annotations, the\n","participants also wore a body camera during daytime, and used sleep diaries to\n","register their sleep times. Each participant was recorded for roughly 24 hours.\n","The accelerometer was an Axivity AX3 wrist watch (image above) that mearures\n","acceleration in all three axes (x, y, z) at a sampling rate of 100Hz.\n","The body camera was a Vicon Autographer with a sampling rate of 1 picture every 20 seconds.\n","Note that the camera images are not part of the data release &mdash; only the\n","raw acceleration trace with text annotations are provided.\n","\n","## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","from glob import glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from imblearn.ensemble import BalancedRandomForestClassifier\n","from sklearn import decomposition\n","from sklearn import preprocessing\n","from sklearn import manifold\n","from sklearn import metrics\n","from tqdm.auto import tqdm\n","from joblib import Parallel, delayed\n","\n","import utils\n","\n","# For reproducibility\n","np.random.seed(42)\n",""]},{"cell_type":"markdown","metadata":{},"source":["## Load and inspect the dataset\n","\n","To run this notebook, you'll need the\n","[Capture-24 dataset](https://ora.ox.ac.uk/objects/uuid:99d7c092-d865-4a19-b096-cc16440cd001).\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Path to capture24 dataset\n","CAPTURE24_PATH = 'capture24/'\n","\n","# Let's see what's in it\n","print(f'Content of {CAPTURE24_PATH}')\n","print(os.listdir(CAPTURE24_PATH))\n","\n","# Let's load and inspect one participant\n","data = utils.load_data(CAPTURE24_PATH+'P001.csv.gz')\n","print('\\nParticipant P001:')\n","print(data)\n","\n","print(\"\\nAnnotations in P001\")\n","print(pd.Series(data['annotation'].unique()))\n",""]},{"cell_type":"markdown","metadata":{},"source":["The annotations are based on the [Compendium of Physical\n","Activity](https://sites.google.com/site/compendiumofphysicalactivities/home).\n","In total, there were more than 200 distinct annotations in the whole dataset.\n","As you can see, the annotations can be very detailed.\n","\n","To develop a model for activity recognition, let's chunk the data into windows of\n","30 sec. The activity recognition model will then be trained to classify the\n","individual windows."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","X, Y, T = utils.make_windows(data, winsec=30)\n","print(\"X shape:\", X.shape)\n","print(\"Y shape:\", Y.shape)\n","print(\"T shape:\", T.shape)\n",""]},{"cell_type":"markdown","metadata":{},"source":["As mentioned, there can be hundreds of distinct annotations, many of which are\n","very similar (e.g. \"sitting, child care\", \"sitting, pet care\").\n","For our purposes, it is enough to translate the annotations into a simpler\n","set of labels. The provided file *annotation-label-dictionary.csv*\n","contains different annotation-to-label mappings that can be used."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ANNO_LABEL_DICT_PATH = CAPTURE24_PATH+'annotation-label-dictionary.csv'\n","anno_label_dict = pd.read_csv(ANNO_LABEL_DICT_PATH, index_col='annotation', dtype='string')\n","print(\"Annotation-Label Dictionary\")\n","print(anno_label_dict)\n","\n","# Translate annotations using Willetts' labels  (see paper reference at the bottom)\n","Y = anno_label_dict.loc[Y, 'label:Willetts2018'].to_numpy()\n","\n","print('\\nLabel distribution (Willetts)')\n","print(pd.Series(Y).value_counts())\n",""]},{"cell_type":"markdown","metadata":{},"source":["We observe some imbalance in the data. This will likely be an issue\n","later for the machine learning model.\n","\n","# Visualization\n","Visualization helps us get some insight and anticipate the difficulties that\n","may arise during the modelling.\n","Let's visualize some examples for each activity label."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NPLOTS = 5\n","unqY = np.unique(Y)\n","fig, axs = plt.subplots(len(unqY), NPLOTS, sharex=True, sharey=True, figsize=(10,10))\n","for y, row in zip(unqY, axs):\n","    idxs = np.random.choice(np.where(Y==y)[0], size=NPLOTS)\n","    row[0].set_ylabel(y)\n","    for x, ax in zip(X[idxs], row):\n","        ax.plot(x)\n","        ax.set_ylim(-5,5)\n","fig.tight_layout()\n","fig.show()\n",""]},{"cell_type":"markdown","metadata":{},"source":["From the plots, we can already tell it should be easier to classify \"sleep\"\n","and maybe \"sit-stand\", with the signal variance being a good discriminative\n","feature for this.\n","Next, let's try to visualize the data in a scatter-plot.\n","The most standard approach to visualize high-dimensional points is to\n","scatter-plot the first two principal components of the data.\n","\n","## PCA visualization\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def scatter_plot(X, Y):\n","    unqY = np.unique(Y)\n","    fig, ax = plt.subplots()\n","    for y in unqY:\n","        X_y = X[Y==y]\n","        ax.scatter(X_y[:,0], X_y[:,1], label=y, alpha=.5, s=10)\n","    fig.legend()\n","    fig.show()\n","\n","print(\"Plotting first two PCA components...\")\n","scaler = preprocessing.StandardScaler()  # PCA requires normalized data\n","X_scaled = scaler.fit_transform(X.reshape(X.shape[0],-1))\n","pca = decomposition.PCA(n_components=2)  # two components\n","X_pca = pca.fit_transform(X_scaled)\n","scatter_plot(X_pca, Y)\n",""]},{"cell_type":"markdown","metadata":{},"source":["## t-SNE visualization\n","PCA's main limitation is in dealing with data that is not linearly separable.\n","Another popular high-dimensional data visualization tool is _t-distributed\n","stochastic neighbor embedding_ (t-SNE).  Let's first use it on top of PCA to\n","visualize 50 principal components.\n","\n","*Note: this may take a while*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Plotting t-SNE on 50 PCA components...\")\n","pca = decomposition.PCA(n_components=50)  # 64 components this time\n","X_pca = pca.fit_transform(X_scaled)\n","tsne = manifold.TSNE(n_components=2,  # project down to 2 components\n","    init='random', random_state=42, perplexity=100, learning_rate='auto')\n","X_tsne_pca = tsne.fit_transform(X_pca)\n","scatter_plot(X_tsne_pca, Y)\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Feature extraction\n","Let's extract a few signal features for each window.\n","Feel free to engineer your own features!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def extract_features(xyz):\n","    ''' Extract features. xyz is an array of shape (N,3) '''\n","\n","    feats = {}\n","    feats['xMean'], feats['yMean'], feats['zMean'] = np.mean(xyz, axis=0)\n","    feats['xStd'], feats['yStd'], feats['zStd'] = np.std(xyz, axis=0)\n","    v = np.linalg.norm(xyz, axis=1)  # magnitude stream\n","    feats['mean'], feats['std'] = np.mean(v), np.std(v)\n","\n","    return feats\n","\n","X_feats = pd.DataFrame([extract_features(x) for x in X])\n","print(X_feats)\n",""]},{"cell_type":"markdown","metadata":{},"source":["Let's visualize the data again using t-SNE, but this time using the extracted\n","features rather than the principal components.\n","\n","*Note: this may take a while*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Plotting t-SNE on extracted features...\")\n","tsne = manifold.TSNE(n_components=2,\n","    init='random', random_state=42, perplexity=100, learning_rate='auto')\n","X_tsne_feats = tsne.fit_transform(X_feats)\n","scatter_plot(X_tsne_feats, Y)\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Activity classification\n","Le fun part. Let's train a balanced random forest on the extracted features to\n","perform activity classification. We use the implementation from\n","[`imbalanced-learn`](https://imbalanced-learn.org/stable/) package, which has\n","better support for imbalanced datasets."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["clf = BalancedRandomForestClassifier(\n","    n_estimators=1000,\n","    replacement=True,\n","    sampling_strategy='not minority',\n","    n_jobs=4,\n","    random_state=42,\n",")\n","clf.fit(X_feats, Y)\n","Y_pred = clf.predict(X_feats)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('\\nClassifier performance in training set')\n","print(metrics.classification_report(Y, Y_pred, zero_division=0))\n","\n","fig, axs = utils.plot_compare(T, Y, Y_pred, trace=X_feats['std'])\n","fig.show()\n",""]},{"cell_type":"markdown","metadata":{},"source":["The classification performance is very good, but this is in-sample! Let's load\n","another subject to test and get the true (out-of-sample) performance."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Load another participant\n","data2 = utils.load_data(CAPTURE24_PATH+'P002.csv.gz')\n","X2, Y2, T2 = utils.make_windows(data2, winsec=30)\n","Y2 = anno_label_dict.loc[Y2, 'label:Willetts2018'].to_numpy()\n","X2_feats = pd.DataFrame([extract_features(x) for x in X2])\n","Y2_pred = clf.predict(X2_feats)\n","\n","print('\\nClassifier performance on held-out subject')\n","print(metrics.classification_report(Y2, Y2_pred, zero_division=0))\n","\n","fig, axs = utils.plot_compare(T2, Y2, Y2_pred, trace=X2_feats['std'])\n","fig.show()\n",""]},{"cell_type":"markdown","metadata":{},"source":["As expected, the classification performance is much worse out of sample, with\n","the macro-averaged F1-score dropping from .90 to .37.\n","On the other hand, the scores for the easy classes \"sleep\" and \"sit-stand\" remained good.\n","Finally, note that participant P001 didn't have the \"bicycling\" class while\n","participant P002 didn't have the \"vehicle\" class.\n","\n","### Next steps\n","So far we've only trained on one subject. To use the whole dataset, repeat the\n","data processing for each of the subjects and concatenate them. You can use the\n","code below for this.\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def load_all_and_make_windows(datafiles):\n","\n","    def worker(datafile):\n","        X, Y, T = utils.make_windows(utils.load_data(datafile), winsec=30)\n","        pid = os.path.basename(datafile).split(\".\")[0]  # participant ID\n","        pid = np.asarray([pid] * len(X))\n","        return X, Y, T, pid\n","\n","    results = Parallel(n_jobs=4)(\n","        delayed(worker)(datafile) for datafile in tqdm(datafiles))\n","\n","    X = np.concatenate([result[0] for result in results])\n","    Y = np.concatenate([result[1] for result in results])\n","    T = np.concatenate([result[2] for result in results])\n","    pid = np.concatenate([result[3] for result in results])\n","\n","    return X, Y, T, pid\n","\n","# # Uncomment below to process all files\n","# DATAFILES = CAPTURE24_PATH+'P[0-9][0-9][0-9].csv.gz'\n","# X, Y, T, pid = load_all_and_make_windows(glob(DATAFILES))\n","# # Save arrays for future use\n","# os.makedirs(\"processed_data/\", exist_ok=True)\n","# np.save(\"processed_data/X.npy\", X)\n","# np.save(\"processed_data/Y.npy\", Y)\n","# np.save(\"processed_data/T.npy\", T)\n","# np.save(\"processed_data/pid.npy\", pid)\n",""]},{"cell_type":"markdown","metadata":{},"source":["## References\n","**Feature extraction**\n","\n","- [On the role of features in human activity recognition](https://dl.acm.org/doi/10.1145/3341163.3347727)\n","- [A Comprehensive Study of Activity Recognition Using Accelerometers](https://www.mdpi.com/2227-9709/5/2/27)\n","\n","**Papers using the Capture-24 dataset**\n","\n","- [Reallocating time from machine-learned sleep, sedentary behaviour or\n","light physical activity to moderate-to-vigorous physical activity is\n","associated with lower cardiovascular disease\n","risk](https://www.medrxiv.org/content/10.1101/2020.11.10.20227769v2.full?versioned=true)\n","(Walmsley2020 labels)\n","- [GWAS identifies 14 loci for device-measured\n","physical activity and sleep\n","duration](https://www.nature.com/articles/s41467-018-07743-4)\n","(Doherty2018 labels)\n","- [Statistical machine learning of sleep and physical activity phenotypes\n","from sensor data in 96,220 UK Biobank\n","participants](https://www.nature.com/articles/s41598-018-26174-1)\n","(Willetts2018 labels)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}